# Лабороторная работа №1
## Задание:
Получить разложение системы линейных уравнений в виде $Ax = b$ через $QR$ разложение и обратной подстановки.
## Решение
Выразить $A$ как $QR$, где $Q$ - ортогональная матрица, а $R$ - верхнетреугольная. Тогда $Ax = b$ превращается в $QRx = b$. Пользуясь ортогональностью $Q$ умножаем обе части на $Q$<sup>$T$</sup>, 
получаем $Q$<sup>$T$</sup>$QR$ = $Q$<sup>$T$</sup>$b$, а отсюда $R = Q$<sup>$T$</sub>$b$. Так-как $R$ - верхнетреугольная матрица, простой обратной подстановкой получаем решение системы.

Код можно открыть в папке labs или же по данной ссылке.

## Основные функции c объяснениями

### QR разложение
```python
def QR_Decomposition(A):
    n, m = A.shape

    Q = np.empty((n, min(n, m)))
    u = np.empty((n, min(n, m)))

    # Получаем первый нормированный и в будущем ортогональный вектор
    u[:, 0] = A[:, 0] 
    Q[:, 0] = u[:, 0] / np.linalg.norm(u[:, 0])

    for i in range(1, min(n, m)):

        u[:, i] = A[:, i]
        for j in range(i):
            # Вычитание из u проекции уже полученных "базисных" векторов
            u[:, i] -= (u[:, i] @ Q[:, j]) * Q[:, j] 

        Q[:, i] = u[:, i] / np.linalg.norm(u[:, i]) # Нормировка

    R = np.zeros((min(n, m), m))
    
    # for цикл для заполнения верхнетреугольной матрицы
    for i in range(min(n, m)):
        for j in range(i, m):
            R[i, j] = A[:, j] @ Q[:, i] # заполнение матрицы R

    return Q, R
```
#### Обратная подстановка
```python
def back_substitution(U, y): # Метод обратной подстановки

    n = U.shape[0] # Получение количества строк
    x = np.zeros_like(y, dtype=np.double); # Создаем массив для хранения решений

    x[-1] = y[-1] / U[-1, -1] # Считаем значение последнего аргумента(тривиальный случай)

    for i in range(n-2, -1, -1):
        # Идем с конца, вычитаем уже посчитанные значения помножив на коэффициенты, находим очередной элемент
        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i] 

    return x
```

#### Получение решения
```python
Q, R = QR_Decomposition(A)
x = back_substitution(R, Q.T @ np.array([1.0, 1.0, 1.0])) # Пример вектора свободных коэффициентов, он может быть абсолютно другим
```

## Сравнение отличия полученного решения от ответа numpy
```python
x_np = np.linalg.solve(A, np.array([1.0, 1.0, 1.0]))
print(f"Difference: {np.linalg.norm(x - x_np)}")
```
Во всех случаях вектор свободных коэффициентов состоит из 1, также рассматриваются только квадратные матрицы, потому что иные numpy не умеет решать (мой код умеет)

1) Матрица $А$ 4x4:
```
0.30939573 0.77221547 0.06561789 0.06056836
0.65572369 0.03470928 0.1384981  0.85681125
0.23034825 0.47891308 0.26702956 0.89660618
0.55875364 0.7569453  0.58536797 0.3969105 
```

Разница с numpy: $1.5700924586837752e-16$

2) Матрица $A$ 5x5:
```
0.54800567 0.96700363 0.87327335 0.53604799 0.58030334
0.78752532 0.47108629 0.75541675 0.3131805  0.04820443
0.71722557 0.20144886 0.59885453 0.7487091  0.01226907
0.18721525 0.77736826 0.6525981  0.65604588 0.23103057
0.26788103 0.55770795 0.26610504 0.33358715 0.38085661
```
Разница с numpy: $1.3104864908000505e-14$

3) Матрица $A$ 6x6:
```
0.74781101 0.53795043 0.45678845 0.83831929 0.8063799  0.46172189
0.57568006 0.08691133 0.58085471 0.49674695 0.7818912  0.09148341
0.05315993 0.06900811 0.27297001 0.70066313 0.58328045 0.5429318 
0.85003837 0.36705692 0.75255499 0.51026664 0.82412013 0.77175606
0.91746383 0.97000156 0.4620011  0.51062645 0.16000186 0.39558734
0.82721815 0.73959694 0.81584512 0.65040122 0.96468092 0.46095654
```

Разница с numpy: $2.7482106012206372e-14$

